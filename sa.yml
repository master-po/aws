aws

ec2
    user data
        commands to be run when instance is initiated
    on-demand
        pay for what you use
        highest cost but not upfront payment
        no long term committment
        recommended for short-term and uninterrupted workloads with predictable application behavior
    reserved instance
        up to 75% discount compared to on-demand
        pay upfront
        1 or 3 year reservation
        reserve a specific instance type
        good for databases
    convertible reserved instance
        RI but can change instance type
        up to 54$ discount
    scheduled reserved instance
        launch within a reserved window
        only reuired to run periodically
    spot instances
        up to 90% discount compared to ondemand
        spot pricing is variable and a max price is set for the workloads
        workload won't run if spot pricing exceeds max
        can be one-time or persistent
            one-time runs once
            persistent runs whenevr price is below threshold
        best for workloads that are resilient to failure
            batch jobs
            data analysis
            image processing
            additional resources for peak usage
            not databases or critical jobs
        spot block
            spot instance that is reserved for 1-6 hours
        spot fleet
            pools of various spot instances can be selected from to get best pricing   
    dedicated host
        dedicated hardware
        provides user with visibility into underlying sockets/cores
        3 year reservation
        can be useful for PITA licensing
        good for strong compliance/security needs
    instance type
        R - good for apps that require a lot of RAM
        C - good for apps that require a lot of 
        M - good for medium apps such as web apps
        I - good for apps that require high storage IO
        G - good for apps that require GPU
        T2/T3 - busrtable instances
            utilizes burst credits
            credits accumulate over time
            similar to QoS tokens
        T2/T3 unlimited - unlimited burst
            no limit on bursts but there is a charge when it happens
    http://169.254.169.254/latest/meta-data - used for an EC2 instance to learn about itself
        lots of URIs after meta-data are available

AMI
    amazon machine image
    AMIs from  amazon marketplace can be selected or build your own
    right click on EC2 instance and choose create image
    stored in a specific region
    AMIs can be shared with another account using their account number
    sharing does not change ownership
    if they copy the AMI to another region, they own the copy
    you cant copy an encyrpted AMI that was shared with you
    you cant copy an AMI with an assocaited billingproduct code that was shared with you
    permit/deny copying AMI is controller by "create volume" check box on the permissions window
    golden AMI - image created with dependencies and applications installed for quicker instantiation


placement groups
    cluster
        ec2 instances are in same AZ and rack
        low-latency
        decreased availability due to lack of geographic diversity
    spread
        spans instances across multiple availability zones
        limited to 7 instances per AZ placement group
        used to maximize availability
    partition
        instances can be partitioned within a group
        up to 7 partitions per AZ
        used for big data apps like HDFS HBase, Cassandra, Kafka

elastic network interfaces - ENI
    basically a vnic
    can have primary ipv4 addr and 1 or more secondary IP
    can have a elastic public IP
    bound to a specific AZ
    interfaces can be created and assiigned/re-assigned between instances

ec2 hibernate
    preserves memory state
    does not reboot  OS
    only available in C M and R instance types
    max 150 GB RAM supported
    no supported on bare metal instances
    must use encrypted EBS
    on-demand and reserved instances only

ELB
    classic
        http(s) and tcp
        mostly deprecated
    application
        http(s) and websocket
        layer 7 only
        route traffic based on URL path, hostname, or query string
        good fit for containers and microservices
        would need mutliple classic load balancers to accomplish same tasks
        can route traffic to EC2 instances, ECS tasks, lambda functions, and private IP addresses
        health checks are performed at target group level
        server will receive client info in x-forwarded-for and x-forwarded-proto http headers
    network
        TCP TLS and UDP
        layer 4 load balancer
        about 100ms latency vs 400ms on ALB
        has static IP
            application and classic do not
        used for best performance
        must allow traffic from outside to access targets
            traffic appears to come from source, not LB
    common security group allows 80/443 from inet to LB  and then LB to the servers
    can scale but not instantaneously
        call aws for "warm-up" if needed
    stickiness
        uses cookies to ensure a client always reaches the same target
        maintains sessions
        might cause load imbalances
        works with classic and application load balancers
        enable by editing attributes in target group settings
    cross-zone load balancing
        enables load balancers to forward traffic to another AZ
        configured in attributes of lb
        classic lb
            disabled by default
            no charges for inter-AZ
        application lb
            always on, cant be disabled
            no charges for inter-AZ
        network lb
            disabled by default
            charges for inter-AZ traffic
    SNI - server name indication
        allows you to run multiple certs on the same lb
        supported for ALB and NLB, not CLB
        multiple listeners are configured for multiple  certs
    connection draining****
        instance de-registers itself
        used to give server time to process a request

ASG - auto scaling group
    scales ec2 instances out and in based on load within a defined range
    attributes
    launch configuration
        AMI + instance type
        EC2 user data
        EBS volumes
        Security groups
        SSH key pair
        min/max/initial size
        network + subnet info
        load balancer info
        scaling policies
    uses cloudwatch alarms to track metrics and trigger scaling
        metrics are computed for thee overall ASG instances
    possible rules
        target average CPU
        number of requests on ELB per instance
        average network in/out
        custom metric such as number of users
            metric is sent to cloudwatch via cloudwatch putmetric API
        schedule-based
    ASGs are free
    configured through launch config or templates
    IAM roles assigned to ASG will get assigned to EC2 instances
    terminates and creates new instances
        can be based  on  LB marking an instance as unhealthy
    target tracking scaling
        simple and easy setup
        example - avg cpu should be around 40%
    simple / step scaling
        add instances when upper threshold is reached
        remove instances when lower threshold is reached
    scheduled actions
        used when growth is anticiapted
        good for predictable capacity changes
    scaling cooldown
        default is 300 seconds
        used to ensure instances arent added/removed before previous scaling activity takes effect
        group-specific overrides default
        scale-in may be configured different from out since in is faster
    default termination policy
        removes instances from AZ with most  instances
        within an az, terminate the instance with oldest launch config/template
    lifecycle hooks
        when an instance is being created or terminated, \
        tasks can happen while in pending/terminating state
    templates should be used over configs
        templates can have versioning
        can have parameter subsets
        can provision on-demand or spot instances
        can use T2 unlimited burst

EBS
    network drive
    root volume is lost when an instance is terminated
    can be detached and re-attached to instances easily
    locked into single availability zone 
    can only be attached to one instance at a time
    GP2
        general performance SSD
        burstable like T2 instance
        3000-16000 IOPS
        up to 16TB
        IOPS is based on  volume siize
    IO 1
        highest performane SSD
        large database workloads
        up to 16TB
        generally max IOPS is 32,000 (nitro up to 64,000)
        IOPS is configurable
        max ratio of 50:1 IOPS to volume size
    ST 1
        low cost HDD volumes
        good for big data
        max IOPS 500
        throughput optimized - max throughput 500 MB/s
    SC 1
        lowest cost HDD
        up to 16TB
        max throughput 250 MB/s
    Only GP2 and IO1 can be root volumes
    lsblk - amazon linux command to view volumes
    snapshot
        incremental - onlt changed blocks are backed up
        resource intensive - done use during heavy load
        stored in s3 but you cant see them
        max 100K snapshots per account
        can be copied across AZ or region
        AMI can be made from snapshot
        can be used for faster instantiation of new services
    migration
        create snapshot from volume and then copy to another region/AZ
    encryption
        data at rest, data in flight, snapshots, volumes from snapshot are all encrypted \
            when encryption is enabled
        minimal impact on latency
        uses keys from KMS
        encrypt unencrypted volume
            create snapshot
            encrypt snapshot
            create new volume from snapshot
            attach to instance
    instance store
        drive physically attached to disk
        better IO performacne than EBS
        survives reboots
        good for cachiing
        data is lost on reboot or termination
        cant resize instance store
        backups would need to be performed by user
        up to 7.5 TB
    RAID
        EBS is already redundant
        can be used to  increase performance or mirror volumes
        0 and 1 are only recommended RAID levels
        can give you up to 100K IOPS

EFS
    elastic file system
    NFS - POSIX filesystem
    not locked into single AZ like EBS
    3x cost GP2
    network file system that can be mounted on any EC2
    can be linked to multiple EC2 instances at the same time
        EBS can  obnly be  linked to one
    use security groups to control access
    only works for linux based AMI
    multiple storage tiers
        cold data can be used to EFS IA - infrequent access
    amazon-efs-utils is a package used to connect EFS shares

RDS
    continuous backups (point in time restore)
        daily full backups (defined by user)
        transaction logs backed up every 5 minutes
        7 day retention but can be increased to 35 days
    snapshots
        manually triggered by user
        can be retained indefinitely
        can be used for instantiating another inatance quickly
    underlying OS is patched for you
    monitoring dashboards
    multii AZ setup for DR
    storage backed by EBS (gp2 or io1)
    vertical and horizontal scaling
    postgres - port 5432
    mysql - port 3306
    mariadb - 3306
    oracle - 1521
    mssql - 1433
    aurora - 3306 or 5432 depending on mode
    read replicas
        up to 5 can be created
        can be  within AZ, cross-AZ, or cross-region
            can be multi-AZ for DR
        eventually consistent asynchronous replication
        commonly used for reporting applications
        cross AZ network traffic will be charged
    multi AZ (DR)
        synchronous replication 
        works in active/standby pair
        failover database will become active during a failure
        not used for scaling
    encryption
        at-rest
            encrypt with KMS
            has to be defined at launch time
            read replicas cant be encrypted if master isnt
            TDE (transparent data encryption) is available for oracle and sql
        in-flight
            uses SSL certificates
            enforce SSL
                prostgresql and mysql require commands to configure
        snapshot will have same encryption state as db
        database can be encrypted by making a snapshot, encrypting, then restoring
    IAM policies are used to manage RDS
        users are within database
    IAM-based authentication works with mysql and postgresql
    IAM roles can be used for authentication
    Aurora
        proprietary and not open source
        optimized for AWS cloud
        compatible with mysql and postgresql tools
        storage grows in 10GB inicrements up to 64TB
        up to 15 replicas
        costs more but more efficient
        6 copies of data are stored across 3 AZ
        very high availability
        only one master can be written to
            writer endpoint is used to send writes to, it will forward to active db
        reader endpoint can read from any replica
        backtrack
            restore data from any point in time without backups/snapshots
        serverless
            automated instantiation
            auto-scaling (horizontal) based on usage
            good for infrequent, intermittent, unpredictable workloads
            no capacity planning needed
            pay per second
            can be more cost effective
            client will connect to proxy fleet which connects to a database
        global database
            1 primary region
            up to 5 secondary (read-only) regions
            replicaition lag is <1s
            up to 16 read replicas per secondary region

Elasticache
    in-memory database cache
    high performance low-latency
    helps reduce load from intensive workloads
    helps make application stateless
    write scaling uses sharding
        can store session data
        can use dynamoDB as alternative
    read scaling uses replicas
    mult AZ failover capability
    query first goes to elasticache, if the data isnt there RDS is queried
    cache hit - queried data was in cache
    cache miss - queried data was not in cache
    queried data is cached after a cache miss
    helps with reads
    user session store
        shares user state data between instances?
    does not support IAM auth
    supports in-flight encryption
    lazy loading
        all accessed data is cached
        mey get stale data
    write through***
        add or update data to cache when written
    session store
        data is stored temporarily but with a TTL for expiration
    REDIS
        mutli AZ failover
        supports read replicas
        data durability with AOF
            makes data persistent through reboots
        back up and restore features
        Redis AUTH
            password/token for accessing a redis cluster
        use security groups for auth
    memcached
        uses sharding
        non persistent cache
        no backup/restore features
        multi-threaded architecture
        SASL based authentication

Route 53
    supports public records
    supports private internal records
    $0.50 per month per hosted zone
    can use domains from 3rd party registrars
    Alias record
        similar to CNAME
        points to a AWS resource
        can be pointed to a root domain
        CNAME cant be pointed to a root domain
    simple routing policy
        cant have health checks attached
        can return more than one IP
            used for client-side load-balancing
    weighted routing policy
        controls percent of requests to go to each resource
        weights are added together and a percentage is derived
        health checks can be assigned
    latency
        dns should respond with closest record in nearest region
    health checks
        default threshold is 3 health checks passed/failed
        default check interval is 30 seconds from 15 health checkers
            2 second average between checks
        HTTP(S) and TCP health checks available
        no charge up to 50 endpoints
        more cost for advanced checks
    failover routing policy
        primary, secondary, and health checks are created
        primary is provided as long as it is healthy
        no health check is required for secondary record
    geo location
        routes based on user location
        countries and continents can be specified
        defualt can also be specified if it is not in a specific country
    multi value
        can return up to 8 records
        can be associated with health checks
        similar to simple for more features

Beanstalk
    developer centric view of deploying an application
    no cost directly, just for underlying services
    user does not need to manage instances, just code
    single instance
        good fior development
    load balancer + asg
        good for production web apps
    asg only
        good for non-web apps in production
    application, version, and environment name are components
    many languages supported - go php ruby, java, docker, etc
    load balancing, asg, ec2, rds, security groups can all be configured automatically
    
S3
    global service but buckets are region specific
    buckets are created and store objects
    key is full path - s3://bucket/folder/file.txt
    key is prefix + object name - folder/file.txt
    object name - name of file - file.txt
    technically there are no directories
        just keys with long names that contain slashes
    max object size is 5TB (5K GB)
    key:value pairs can be added to objects
    tags can also be added to objects also
    versioning IDs can be added to objects
    files existing before versioning is enabled will have a value of null
    suspending versioning does not delete previous versions
    must use multi-part upload for files larger than 5GB
        recommended for files larger than  100MB
        can help parallelize uploads (increase upload speed)
    transfer acceleration
        increases transfer speed by transferring file to AWS edge location and then internally to S3
    byte range fetch
        parallelize get requests - increase download speed
        can also be used to download a portion of a file such as the header
    select and glacier select
        you can use sql queries to select a portion of a file such as a csv to minmize downloads/reads
        server-side  filtering will happen
        less network transfer and CPU usage
    event  notifications
        event targets can be SNS, SQS, or lambda
    athena
        serverless service to query s3 files in SQL format
        has ODBC/JDBC drivers
        BI, analytics, logging, reporting, flows, etc
    locking
        s3 object lock / glacier vault lock
        object can't be modified or deleted once created
        used for compliance and data retention
    encryption
        SSE-S3
            server side encryption AES256
            must use header "x-amz-server-side-encryption" - "AES256"
            S3 will encrypt the data
            key is owned and maintained by amazon
        SSE-KMS
            customer controls keys
            customer has audit trail
            object is encrypted server side
            must use header "x-amz-server-side-encryption" "aws:kms"
                uses KMS customer master key
            5-30K KMS requests per second (depending on location) allowed for storage
        SSE-C
            server side encryption using your own keys from outside AWS
            amazon does not store the key
            HTTPS must be used
            encryption key must be provided in HTTP headers for every HTTP request made
        client-side encryption
            client encrypts data before sending it to AWS
        SSE-S3 and SSE-KMS can be enabled as default options
    security
        IAM policies
        bucket policies
            bucket-wide rules 
        object ACL
        bucket ACL
        IAM principal can access resource if IAM policy OR buckey policy allows it
        explicit deny will block
        block public access
            used to prevent data leaks
            prevents any object from becominig public
        MFA delete can be enabled which requires MFA to delete a version
        pre-signed URLs can be used to enable access to a resource for a limited amount of time
    S3 website
        <bucket name>.s3-website.<AWS region>.amazonaws.com
        <bucket name>.s3-website-<AWS region>.amazonaws.com
        403 error indicates no public access
        configure static website hosting in bucket properties
    CORS
        cross origin resource sharing
        origin - scheme, host, and port
            https://www.google.com
            https:// - scheme
            www.google.com - host
            port is 443 by default
        webpage is received that instructs client to perform additional GET request on another site to access content
        needs to be specifically enabled on cross origin resource to work, even is object is public
    consistency model
        s3 is eventually consistent
        as soon as object is written, it can be read
            if object was requested and unavailable before write, it may take time to update
        GET may fetch an older version if very soon after PUT
        object may be readable shortly after a delete
    MFA Delete
        requires MFA to do important operations on S3
        versioning must be enabled
        only bucket owner can enable/disable MFA delete
        can only be enabled via CLI
    Access logs
        all S3 requests will be logged
        logging bucket should be separate from monitored bucket
            infinite loop will be caused if you dont do this
    Replication
        CRR - cross region replication
        SRR - same region replication
        asynchornous replication
        can be copied to another account
        only new objects are replicated
        delete operations are not replicated
        no replication chaining from a replicated bucket to another bucket
        entire bucket or specific prefix/tags can be replicated
    Pre-signed URLs
        could be used to prevent users from having a permanent URL
        temporarily allow user to upload file
        can be generated via CLI or SDK
            downloads via CLI
            uploads viua SDK
        default timeout for URL is 3600s (one hour)
    Standard - general purpose
        high durability (11 x 9s)
        works across multiple AZs
        5x 9 availability
        can sustaiin 2 facility failures
        big data, mobilie apps, content distribution
    Standard IA
        infrequently accessed but does require rapid access
        lower cost than s3
        same durability/reliability as standard
    One zone IA
        same as IA but stored in one AZ
        data lost iif one AZ is destroyed
        used for backup data or data that can be recreated
    Intelligent tiering
        moves objects between tiers based on access pattern
        small monthly monitoring fee
    Glacier
        low cost storage for archiving/backup
        data is retained long term (10+ years)
        very low cost per month + retrieval cost
        stored in vault which is similar to buckets
        retrieval options
            expedited 1-5 minutes
            standard 3-5 hours
            bulk 5-12 hours
        minimum storage durationi of 90 days
        deep archive
            even cheaper than glacier
            retrieval options
                standard 12 hours
                bulk 48 hours
            minimum storage duration 180 days
    lifecycle rules
        defines how to automaticallt move objects between tiers or delete based on age/access

CLI
    aws configure - allows you to enter keys and default settings
    EC2 instances should use IAM roles, not your key credentials for authenticated actions
    aws s3 mb s3://name - makes a bucket
    aws s3 rb s3://name - removes bucket

SDK
    recommended to use default credential provider chain
    do not store credentials in code
    may rely on instance profile credentials
    aws credentials at ~/.aws/credentials for on-prem computers
    assists with rate-limiting through exponential backoff

AWS Policy
    resource - resource providing access to
    polcy simulator - kind of like packet tracer. analyzes what policy impact will be

Cloudfront
    protects from DDoS attacks
    creates CDN for data
    origins
        s3
            distribute files and cache at edge
            Cloudfront OAI - origin access identity
                IAM role for cloudfront to access bucket
            can also be used as ingress upload to s3
        custom origin
            any http enndpoint
            ALB, EC2, S3 website, any http backend
    geo restrictions
        white/blacklist based on country/region/etc
    vs cross region replication
        cloudfront is global, files are cached for a day or so after retrieval, great for static content and global access
        CRR must be setup for each region, file are updated real-time, read-only, great for dynamic content which
            requires access in a few regions
    signed url / cookie
        expires after a period of time
            could be minutes to years
        includes which IP data will be accessed from
        url can be used for a single file or path
        cookies can be used for multiple files
        cloudfront signed url different than s3 pre-signed url
            cf has more features and could be http in addition to object
            cf can filter by ip range
            cf needs to be used if cf is in use
            pre-signed allows direct access to bucket

global accelerator
    traffic is sent to nearest edge location across AWS insternal network to resource
    creates 2 anycast IPs for your application
    works with elastic IP, ec2, ALB, NLB
    does not server cached content from edge like cloudfront

snowball
    physical data transport for moving lots of data to aws cloud
    use if it takes over a week to transfer your data to the network
    install snowball client on to servers, copy data, ship to amazon
    edge
        adds compute capability to device
        can run ec2 or lambda functions
    snowmobile
        used for exabytes of data
        a literal truck
        100 PB capacity
    cant import into glacier directly
        import into s3 then use lifecycle policy to move to glacier
    
storage gateway
    used for hybrid cloud environments
        DR, backup, tiered storage
    file gateway
        runs on-prem
        caches s3 storage localy
        used to make s3 buckets available using NFS or SMB
        supports S3 standard IA and one-zone IA
        bucket uses IAM roles for each gateway
        data is cached in file gateway
        can be mounted on many servers
    volume gateway
        block storage using iSCSI backed by S3
        backed by EBS snapshots which can resotre on-prem volumes
        cached volumes provide low latency access to recent data
        stored volumes - entire dataset is on-prem with scheduled backups to s3
        on-prem volume gateway is accessed  via iSCSI
    tape gateway
        tape gateway exists on-prem and uses iSCSI
        data is backed up to virtual tapes in s3 and archived tapes in glacier
    hardware appliance is available 

FSx
    for windows
        fully managed windows file system shared drive
        support SMB and NTFS
        AD integration
        built on SSD
        can be multi-AZ for HA
        backed up daily to S3
    for lustre
        linux + cluster = lustre
        used for large scale computing, HPC, machine learning
        very high performance
        seamless integration with s3
        can be used from on-prem servers

SQS
    facilitates asynchronous communication
    used to decouple applications
    unlimited throughput and number of messages in queue
    default message retention is 4 days, max 14 days
    < 10  ms response time
    256KB max message size
    may have duplicate and out of order messages (at least once delivery, best effort)
    consumers pull data
    consumers can poll up to 10 messages at a time
    consumers should delete messages using deletemessage API
    throughput is not provisioned
    encryption in flight using HTTPS
    encryption at rest via KMS
    IAM policies are used to control access to SQS API
    SQS access policies can also be used and are similar to S3 bucket policies
    Message visibility timeout
        period of timie where message becomes "invisible" to consumers after it has been polled
        message will become visible again if it is not deleted during the timeout period
        ChangeMessageAvailability can be adjusted to change this
    dead letter queue
        when a consumer fails to process a message during the timeout it is moved back to the queue
        thresholds can be set for how many time this^ can happen
        once threshold is reached, message is moved to dead letter queue
        useful for debugging
        DLQ should have high retention such as 14 days
        configured like a normal queue, other queues sppecify it as their DLQ
    delay queue
        default is 0 but can be configured up to 15 minutes
        default can be defined at queue level or inidividually with the DelaySeconds parameter
        delays messages - not sure what use case is
    FIFO Queue
        first messages in are first out
        maintains ordering
        limited to 300 msg/s or 3000 msg/s with batching
        name must end with .fifo
        deduplication ID must be specified with message
    ASG integration
        cloudwatch is commonly used  to scale ec2 instances when queue length becomes too long
        custom metric is defined in cloudwatch
        alarm is created to trigger scaling

SNS
    used when messages need to be sent to many receivers
    publisher/subscriber model
    event producer sends message to one SNS topic
    subscribers liisten to topic notifications
    each topic subscriber will get all messages
    data is pushed to subscribers
    up to 10M subscriptions
    100K topic limit
    data is not peristent in queue, will be lost if not delivered
    do not need to provision throughput
    subscribers can be basically anything
    topic publish is standard, but direct publish exists for mobile apps
    in-flight encryption with HTTPS
    at rest encryption with KMS
    AIM policies for access control
    SNS access polices can be used which are similar to s3 bucket policies
    Fan-out
        messages are sent to SNS topic first, then multiple SQS queues subscribe to it
        common to use multiple queues for multiple applications
        SNS can't send messages to FIFO queues
        commonly used with s3 notifications
            s3 notifications can only be sent to one location

Kinesis
    managed alternative to apache kafka
    great for application logs, metrics, IoT, real-time big data, streaming telemetry
    data is auto replicated to 3 AZ
    data is sent to streams, then can be analyzed by analytics, then stored with firehose
    data expires after a set number of days
    access control is delegated through IAM policies
    encryption in flight with HTTPS
    encryption at reast with KMS
    VPC endpoints can be add to make kinesis available within VPC
    throughput must be provisioned
    Kinesis Streams
        low latency streaming ingest at scale
        streams are divided into shards/partitions
        default data retention is 1 day, max is 7 days
        consumers pull data
        unlimited consumers
        data can be replayed
        multiple apps can consume the same stream
        data does not get deleted, it just expires?
        shard
            one stream is many shards
            1MB/s or 1000msg/s write per shard
            2MB/s read per shard
            billing is per shard provisioned
            messages can be batched
            records are ordered per shard
            messages get a sequence number when they are received
            API
                put records are used to send data to kinesis
                must use partition key (shard id)
                partition keys should be chosen in a way that creates distribution across shards
                ProvisionedThroughputExceeded - similar to 429 error, shard is overloaded
                    use backoff timers or more shards to fix
        consumers can use CLI, SDK, or Kinesis clent library for consumption
            KCL uses dynamoDB
    Kinesis Analytics
        used for real time analytics on streams using SQL
    Kinesis Firehose
        loads streams into s3, redshift, elasticsearch, splunk
        fully managed, automatically scales, no administration
        near real-time (60 sec latency minimum for non-full batches)
            or 32MN data at a time
        pay for amount of data going through firehose
        does not store data

MQ
    managed apache activeMQ
    doesnt scale as well as SQS/SNS
    has queues and topics
    good for migrating apps from on-prem to the cloud
    runs on dedicated machine with HA failover
    supports openwire, ws, mqtt, stomp, amqp protocols

Lambda
    runs on-demand
    pay per request and compute time
    free tier - 1M requests and 400,000 GB compute time
        $0.20 for next 1M requests
        billing also based on amout of RAM provisioned
    up to 3GB RAM can be provisioned per function
    node.js, java, python, c#, golang, ruby, powershell, customer runtime API
    can't run docker in lambda
    scales automatically
    errors are output to cloudwatch logs
    128MB to 3TB RAM available
    max execution time is 900s/15m
    max 4KB environment variables
    max 512MB disk capacity in /tmp
        /tmp should be used to load files at startup
    default 1000 concurrency executions (can be increased)
    max deployment size zipped - 50MB
        unzipped - 250MB
    Lambda@edge
        deploys lambda functions with cloudfront instead of in one AZ
        can use lambda to change cloudfront requests and responses
        viewer request (user to cloudfront), origin request (cloudfront to origin), \
            origin response (origin to cloudfront), and viewer response (cloudfront to user) can all be modified with lambda
        responses can be sent to users without communicating with origin (AWS resource)

DynamoDB
    NoSQL - not only sql
    fully managed, highly available across 3 AZ
    can handle millions of requests per second, trillions of rows, 100s of TB of storage
        massively scalable
    uses  IAM for security
    low latency
    auto scaling
    made of tables
    each table has primary key
    tables can infinite rows
    each item has attributes
    maximum item size 400KB
    DMS can be used to migrate from mongo, oracle, mysql, etc
    table must have provisioned read and write capacity units
        RCU - read capacity units
            1 RCU = 1 strongly consistent or 2 eventually consistent read(s) of 4KB/s
        WCU - write capacity units
            1 WCU = 1 write of 1KB/s
        auto-scaling can be configured to meet demand
        throughput can be temporarily exceeded with burst credits
            ProvisionedThroughException is error similar to 429 when burst credits are used
    DAX
        DynamoDB accelerator
        seamless cache for dynamodb
        fixes hotkey problem (too many reads on a specific key)
        5 minute TTL cache by default
        multi AZ
    Streams
        changes in  dynamodb are sent to dynamodb stream
        allows for reacting to changes like new users, analytics, inserting into elasticsearch
        24 hour data retention
    Transactions
        coordinates insert update and delete across multiple tables
        "all or nothing" tables
    on demand
        no capacity planning needed
        WCU RCU scales automatically
        more expensive
        helpful with unpredicatble traffic spikes
    backup/restore
        point in time like RDS
        no performance impact
    global tables
        cross region replication
        must enable streams
        useful for low latency and DR purposes