aws

ec2
    user data
        commands to be run when instance is initiated
    on-demand
        pay for what you use
        highest cost but not upfront payment
        no long term committment
        recommended for short-term and uninterrupted workloads with predictable application behavior
    reserved instance
        up to 75% discount compared to on-demand
        pay upfront
        1 or 3 year reservation
        reserve a specific instance type
        good for databases
    convertible reserved instance
        RI but can change instance type
        up to 54$ discount
    scheduled reserved instance
        launch within a reserved window
        only reuired to run periodically
    spot instances
        up to 90% discount compared to ondemand
        spot pricing is variable and a max price is set for the workloads
        workload won't run if spot pricing exceeds max
        can be one-time or persistent
            one-time runs once
            persistent runs whenevr price is below threshold
        best for workloads that are resilient to failure
            batch jobs
            data analysis
            image processing
            additional resources for peak usage
            not databases or critical jobs
        spot block
            spot instance that is reserved for 1-6 hours
        spot fleet
            pools of various spot instances can be selected from to get best pricing   
    dedicated host
        dedicated hardware
        provides user with visibility into underlying sockets/cores
        3 year reservation
        can be useful for PITA licensing
        good for strong compliance/security needs
    instance type
        R - good for apps that require a lot of RAM
        C - good for apps that require a lot of 
        M - good for medium apps such as web apps
        I - good for apps that require high storage IO
        G - good for apps that require GPU
        T2/T3 - busrtable instances
            utilizes burst credits
            credits accumulate over time
            similar to QoS tokens
        T2/T3 unlimited - unlimited burst
            no limit on bursts but there is a charge when it happens
    http://169.254.169.254/latest/meta-data - used for an EC2 instance to learn about itself
        lots of URIs after meta-data are available
    SR-IOV
        higher bandwidth ec2 instance
        has elastic network adapter - ENA
            up to 100gbps
    elastic fabric adapter
        improved ENA
        only works for linux
        good for lots of internode communications
        bypasses OS to provide higher performance

AMI
    amazon machine image
    AMIs from  amazon marketplace can be selected or build your own
    right click on EC2 instance and choose create image
    stored in a specific region
    AMIs can be shared with another account using their account number
    sharing does not change ownership
    if they copy the AMI to another region, they own the copy
    you cant copy an encyrpted AMI that was shared with you
    you cant copy an AMI with an assocaited billingproduct code that was shared with you
    permit/deny copying AMI is controller by "create volume" check box on the permissions window
    golden AMI - image created with dependencies and applications installed for quicker instantiation


placement groups
    cluster
        ec2 instances are in same AZ and rack
        low-latency
        decreased availability due to lack of geographic diversity
    spread
        spans instances across multiple availability zones
        limited to 7 instances per AZ placement group
        used to maximize availability
    partition
        instances can be partitioned within a group
        up to 7 partitions per AZ
        used for big data apps like HDFS HBase, Cassandra, Kafka

elastic network interfaces - ENI
    basically a vnic
    can have primary ipv4 addr and 1 or more secondary IP
    can have a elastic public IP
    bound to a specific AZ
    interfaces can be created and assiigned/re-assigned between instances

ec2 hibernate
    preserves memory state
    does not reboot  OS
    only available in C M and R instance types
    max 150 GB RAM supported
    no supported on bare metal instances
    must use encrypted EBS
    on-demand and reserved instances only

ELB
    classic
        http(s) and tcp
        mostly deprecated
    application
        http(s) and websocket
        layer 7 only
        route traffic based on URL path, hostname, or query string
        good fit for containers and microservices
        would need mutliple classic load balancers to accomplish same tasks
        can route traffic to EC2 instances, ECS tasks, lambda functions, and private IP addresses
        health checks are performed at target group level
        server will receive client info in x-forwarded-for and x-forwarded-proto http headers
    network
        TCP TLS and UDP
        layer 4 load balancer
        about 100ms latency vs 400ms on ALB
        has static IP
            application and classic do not
        used for best performance
        must allow traffic from outside to access targets
            traffic appears to come from source, not LB
    common security group allows 80/443 from inet to LB  and then LB to the servers
    can scale but not instantaneously
        call aws for "warm-up" if needed
    stickiness
        uses cookies to ensure a client always reaches the same target
        maintains sessions
        might cause load imbalances
        works with classic and application load balancers
        enable by editing attributes in target group settings
    cross-zone load balancing
        enables load balancers to forward traffic to another AZ
        configured in attributes of lb
        classic lb
            disabled by default
            no charges for inter-AZ
        application lb
            always on, cant be disabled
            no charges for inter-AZ
        network lb
            disabled by default
            charges for inter-AZ traffic
    SNI - server name indication
        allows you to run multiple certs on the same lb
        supported for ALB and NLB, not CLB
        multiple listeners are configured for multiple  certs
    connection draining****
        instance de-registers itself
        used to give server time to process a request

ASG - auto scaling group
    scales ec2 instances out and in based on load within a defined range
    attributes
    launch configuration
        AMI + instance type
        EC2 user data
        EBS volumes
        Security groups
        SSH key pair
        min/max/initial size
        network + subnet info
        load balancer info
        scaling policies
    uses cloudwatch alarms to track metrics and trigger scaling
        metrics are computed for thee overall ASG instances
    possible rules
        target average CPU
        number of requests on ELB per instance
        average network in/out
        custom metric such as number of users
            metric is sent to cloudwatch via cloudwatch putmetric API
        schedule-based
    ASGs are free
    configured through launch config or templates
    IAM roles assigned to ASG will get assigned to EC2 instances
    terminates and creates new instances
        can be based  on  LB marking an instance as unhealthy
    target tracking scaling
        simple and easy setup
        example - avg cpu should be around 40%
    simple / step scaling
        add instances when upper threshold is reached
        remove instances when lower threshold is reached
    scheduled actions
        used when growth is anticiapted
        good for predictable capacity changes
    scaling cooldown
        default is 300 seconds
        used to ensure instances arent added/removed before previous scaling activity takes effect
        group-specific overrides default
        scale-in may be configured different from out since in is faster
    default termination policy
        removes instances from AZ with most  instances
        within an az, terminate the instance with oldest launch config/template
    lifecycle hooks
        when an instance is being created or terminated, \
        tasks can happen while in pending/terminating state
    templates should be used over configs
        templates can have versioning
        can have parameter subsets
        can provision on-demand or spot instances
        can use T2 unlimited burst

EBS
    network drive
    root volume is lost when an instance is terminated
    can be detached and re-attached to instances easily
    locked into single availability zone 
    can only be attached to one instance at a time
    GP2
        general performance SSD
        burstable like T2 instance
        3000-16000 IOPS
        up to 16TB
        IOPS is based on  volume siize
    IO 1
        highest performane SSD
        large database workloads
        up to 16TB
        generally max IOPS is 32,000 (nitro up to 64,000)
        IOPS is configurable
        max ratio of 50:1 IOPS to volume size
    ST 1
        low cost HDD volumes
        good for big data
        max IOPS 500
        throughput optimized - max throughput 500 MB/s
    SC 1
        lowest cost HDD
        up to 16TB
        max throughput 250 MB/s
    Only GP2 and IO1 can be root volumes
    lsblk - amazon linux command to view volumes
    snapshot
        incremental - onlt changed blocks are backed up
        resource intensive - done use during heavy load
        stored in s3 but you cant see them
        max 100K snapshots per account
        can be copied across AZ or region
        AMI can be made from snapshot
        can be used for faster instantiation of new services
    migration
        create snapshot from volume and then copy to another region/AZ
    encryption
        data at rest, data in flight, snapshots, volumes from snapshot are all encrypted \
            when encryption is enabled
        minimal impact on latency
        uses keys from KMS
        encrypt unencrypted volume
            create snapshot
            encrypt snapshot
            create new volume from snapshot
            attach to instance
    instance store
        drive physically attached to disk
        better IO performacne than EBS
        survives reboots
        good for caching
        data is lost on reboot or termination
        cant resize instance store
        backups would need to be performed by user
        up to 7.5 TB
    RAID
        EBS is already redundant
        can be used to  increase performance or mirror volumes
        0 and 1 are only recommended RAID levels
        can give you up to 100K IOPS

EFS
    elastic file system
    NFS - POSIX filesystem
    not locked into single AZ like EBS
    3x cost GP2
    network file system that can be mounted on any EC2
    can be linked to multiple EC2 instances at the same time
        EBS can  obnly be  linked to one
    use security groups to control access
    only works for linux based AMI
    multiple storage tiers
        cold data can be used to EFS IA - infrequent access
    amazon-efs-utils is a package used to connect EFS shares

RDS
    continuous backups (point in time restore)
        daily full backups (defined by user)
        transaction logs backed up every 5 minutes
        7 day retention but can be increased to 35 days
    snapshots
        manually triggered by user
        can be retained indefinitely
        can be used for instantiating another inatance quickly
    amazon will provision an EC2 instance for you
    underlying OS is patched for you
    monitoring dashboards
    multii AZ setup for DR
    storage backed by EBS (gp2 or io1)
    vertical and horizontal scaling
    postgres - port 5432
    mysql - port 3306
    mariadb - 3306
    oracle - 1521
    mssql - 1433
    aurora - 3306 or 5432 depending on mode
    read replicas
        up to 5 can be created
        can be  within AZ, cross-AZ, or cross-region
            can be multi-AZ for DR
        eventually consistent asynchronous replication
        commonly used for reporting applications
        cross AZ network traffic will be charged
    multi AZ (DR)
        synchronous replication 
        works in active/standby pair
        failover database will become active during a failure
        not used for scaling
    encryption
        at-rest
            encrypt with KMS
            has to be defined at launch time
            read replicas cant be encrypted if master isnt
            TDE (transparent data encryption) is available for oracle and sql
        in-flight
            uses SSL certificates
            enforce SSL
                prostgresql and mysql require commands to configure
        snapshot will have same encryption state as db
        database can be encrypted by making a snapshot, encrypting, then restoring
    IAM policies are used to manage RDS
        users are within database
    IAM-based authentication works with mysql and postgresql
    IAM roles can be used for authentication
    Aurora
        proprietary and not open source
        optimized for AWS cloud
        compatible with mysql and postgresql tools
        storage grows in 10GB inicrements up to 64TB
        up to 15 replicas
        costs more but more efficient
        automatically scales storage
        6 copies of data are stored across 3 AZ
        very high availability
        only one master can be written to
            writer endpoint is used to send writes to, it will forward to active db
        reader endpoint can read from any replica
        backtrack
            restore data from any point in time without backups/snapshots
        serverless
            automated instantiation
            auto-scaling (horizontal) based on usage
            good for infrequent, intermittent, unpredictable workloads
            no capacity planning needed
            pay per second
            can be more cost effective
            client will connect to proxy fleet which connects to a database
        global database
            1 primary region
            up to 5 secondary (read-only) regions
            replicaition lag is <1s
            up to 16 read replicas per secondary region

Elasticache
    in-memory database cache
    high performance low-latency
    helps reduce load from intensive workloads
    helps make application stateless
    write scaling uses sharding
        can store session data
        can use dynamoDB as alternative
    read scaling uses replicas
    mult AZ failover capability
    query first goes to elasticache, if the data isnt there RDS is queried
    cache hit - queried data was in cache
    cache miss - queried data was not in cache
    queried data is cached after a cache miss
    helps with reads
    user session store
        shares user state data between instances?
    does not support IAM auth
    supports in-flight encryption
    lazy loading
        all accessed data is cached
        mey get stale data
    write through***
        add or update data to cache when written
    session store
        data is stored temporarily but with a TTL for expiration
    REDIS
        mutli AZ failover
        supports read replicas
        data durability with AOF
            makes data persistent through reboots
        back up and restore features
        Redis AUTH
            password/token for accessing a redis cluster
        use security groups for auth
    memcached
        uses sharding
        non persistent cache
        no backup/restore features
        multi-threaded architecture
        SASL based authentication

Route 53
    supports public records
    supports private internal records
    $0.50 per month per hosted zone
    can use domains from 3rd party registrars
    Alias record
        similar to CNAME
        points to a AWS resource
        can be pointed to a root domain
        CNAME cant be pointed to a root domain
    simple routing policy
        cant have health checks attached
        can return more than one IP
            used for client-side load-balancing
    weighted routing policy
        controls percent of requests to go to each resource
        weights are added together and a percentage is derived
        health checks can be assigned
    latency
        dns should respond with closest record in nearest region
    health checks
        default threshold is 3 health checks passed/failed
        default check interval is 30 seconds from 15 health checkers
            2 second average between checks
        HTTP(S) and TCP health checks available
        no charge up to 50 endpoints
        more cost for advanced checks
    failover routing policy
        primary, secondary, and health checks are created
        primary is provided as long as it is healthy
        no health check is required for secondary record
    geo location
        routes based on user location
        countries and continents can be specified
        defualt can also be specified if it is not in a specific country
    multi value
        can return up to 8 records
        can be associated with health checks
        similar to simple for more features

Beanstalk
    developer centric view of deploying an application
    no cost directly, just for underlying services
    user does not need to manage instances, just code
    single instance
        good fior development
    load balancer + asg
        good for production web apps
    asg only
        good for non-web apps in production
    application, version, and environment name are components
    many languages supported - go php ruby, java, docker, etc
    load balancing, asg, ec2, rds, security groups can all be configured automatically
    
S3
    global service but buckets are region specific
    buckets are created and store objects
    key is full path - s3://bucket/folder/file.txt
    key is prefix + object name - folder/file.txt
    object name - name of file - file.txt
    technically there are no directories
        just keys with long names that contain slashes
    max object size is 5TB (5K GB)
    key:value pairs can be added to objects
    tags can also be added to objects also
    versioning IDs can be added to objects
    files existing before versioning is enabled will have a value of null
    suspending versioning does not delete previous versions
    must use multi-part upload for files larger than 5GB
        recommended for files larger than  100MB
        can help parallelize uploads (increase upload speed)
    transfer acceleration
        increases transfer speed by transferring file to AWS edge location and then internally to S3
    byte range fetch
        parallelize get requests - increase download speed
        can also be used to download a portion of a file such as the header
    select and glacier select
        you can use sql queries to select a portion of a file such as a csv to minmize downloads/reads
        server-side  filtering will happen
        less network transfer and CPU usage
    event  notifications
        event targets can be SNS, SQS, or lambda
    athena
        serverless service to query s3 files in SQL format
        has ODBC/JDBC drivers
        BI, analytics, logging, reporting, flows, etc
    locking
        s3 object lock / glacier vault lock
        object can't be modified or deleted once created
        used for compliance and data retention
    encryption
        SSE-S3
            server side encryption AES256
            must use header "x-amz-server-side-encryption" - "AES256"
            S3 will encrypt the data
            key is owned and maintained by amazon
        SSE-KMS
            customer controls keys
            customer has audit trail
            object is encrypted server side
            must use header "x-amz-server-side-encryption" "aws:kms"
                uses KMS customer master key
            5-30K KMS requests per second (depending on location) allowed for storage
        SSE-C
            server side encryption using your own keys from outside AWS
            amazon does not store the key
            HTTPS must be used
            encryption key must be provided in HTTP headers for every HTTP request made
        client-side encryption
            client encrypts data before sending it to AWS
        SSE-S3 and SSE-KMS can be enabled as default options
    security
        IAM policies
        bucket policies
            bucket-wide rules 
        object ACL
        bucket ACL
        IAM principal can access resource if IAM policy OR buckey policy allows it
        explicit deny will block
        block public access
            used to prevent data leaks
            prevents any object from becominig public
        MFA delete can be enabled which requires MFA to delete a version
        pre-signed URLs can be used to enable access to a resource for a limited amount of time
    S3 website
        <bucket name>.s3-website.<AWS region>.amazonaws.com
        <bucket name>.s3-website-<AWS region>.amazonaws.com
        403 error indicates no public access
        configure static website hosting in bucket properties
    CORS
        cross origin resource sharing
        origin - scheme, host, and port
            https://www.google.com
            https:// - scheme
            www.google.com - host
            port is 443 by default
        webpage is received that instructs client to perform additional GET request on another site to access content
        needs to be specifically enabled on cross origin resource to work, even is object is public
    consistency model
        s3 is eventually consistent
        as soon as object is written, it can be read
            if object was requested and unavailable before write, it may take time to update
        GET may fetch an older version if very soon after PUT
        object may be readable shortly after a delete
    MFA Delete
        requires MFA to do important operations on S3
        versioning must be enabled
        only bucket owner can enable/disable MFA delete
        can only be enabled via CLI
    Access logs
        all S3 requests will be logged
        logging bucket should be separate from monitored bucket
            infinite loop will be caused if you dont do this
    Replication
        CRR - cross region replication
        SRR - same region replication
        asynchornous replication
        can be copied to another account
        only new objects are replicated
        delete operations are not replicated
        no replication chaining from a replicated bucket to another bucket
        entire bucket or specific prefix/tags can be replicated
    Pre-signed URLs
        could be used to prevent users from having a permanent URL
        temporarily allow user to upload file
        can be generated via CLI or SDK
            downloads via CLI
            uploads viua SDK
        default timeout for URL is 3600s (one hour)
    Standard - general purpose
        high durability (11 x 9s)
        works across multiple AZs
        5x 9 availability
        can sustaiin 2 facility failures
        big data, mobilie apps, content distribution
    Standard IA
        infrequently accessed but does require rapid access
        lower cost than s3
        same durability/reliability as standard
    One zone IA
        same as IA but stored in one AZ
        data lost iif one AZ is destroyed
        used for backup data or data that can be recreated
    Intelligent tiering
        moves objects between tiers based on access pattern
        small monthly monitoring fee
    Glacier
        low cost storage for archiving/backup
        data is retained long term (10+ years)
        very low cost per month + retrieval cost
        stored in vault which is similar to buckets
        retrieval options
            expedited 1-5 minutes
            standard 3-5 hours
            bulk 5-12 hours
        minimum storage durationi of 90 days
        deep archive
            even cheaper than glacier
            retrieval options
                standard 12 hours
                bulk 48 hours
            minimum storage duration 180 days
    lifecycle rules
        defines how to automaticallt move objects between tiers or delete based on age/access

CLI
    aws configure - allows you to enter keys and default settings
    EC2 instances should use IAM roles, not your key credentials for authenticated actions
    aws s3 mb s3://name - makes a bucket
    aws s3 rb s3://name - removes bucket

SDK
    recommended to use default credential provider chain
    do not store credentials in code
    may rely on instance profile credentials
    aws credentials at ~/.aws/credentials for on-prem computers
    assists with rate-limiting through exponential backoff

AWS Policy
    resource - resource providing access to
    polcy simulator - kind of like packet tracer. analyzes what policy impact will be

Cloudfront
    protects from DDoS attacks
    creates CDN for data
    origins
        s3
            distribute files and cache at edge
            Cloudfront OAI - origin access identity
                IAM role for cloudfront to access bucket
            can also be used as ingress upload to s3
        custom origin
            any http enndpoint
            ALB, EC2, S3 website, any http backend
    geo restrictions
        white/blacklist based on country/region/etc
    vs cross region replication
        cloudfront is global, files are cached for a day or so after retrieval, great for static content and global access
        CRR must be setup for each region, file are updated real-time, read-only, great for dynamic content which
            requires access in a few regions
    signed url / cookie
        expires after a period of time
            could be minutes to years
        includes which IP data will be accessed from
        url can be used for a single file or path
        cookies can be used for multiple files
        cloudfront signed url different than s3 pre-signed url
            cf has more features and could be http in addition to object
            cf can filter by ip range
            cf needs to be used if cf is in use
            pre-signed allows direct access to bucket

global accelerator
    traffic is sent to nearest edge location across AWS insternal network to resource
    creates 2 anycast IPs for your application
    works with elastic IP, ec2, ALB, NLB
    does not server cached content from edge like cloudfront

snowball
    physical data transport for moving lots of data to aws cloud
    use if it takes over a week to transfer your data to the network
    install snowball client on to servers, copy data, ship to amazon
    edge
        adds compute capability to device
        can run ec2 or lambda functions
    snowmobile
        used for exabytes of data
        a literal truck
        100 PB capacity
    cant import into glacier directly
        import into s3 then use lifecycle policy to move to glacier
    
storage gateway
    used for hybrid cloud environments
        DR, backup, tiered storage
    file gateway
        runs on-prem
        caches s3 storage localy
        used to make s3 buckets available using NFS or SMB
        supports S3 standard IA and one-zone IA
        bucket uses IAM roles for each gateway
        data is cached in file gateway
        can be mounted on many servers
    volume gateway
        block storage using iSCSI backed by S3
        backed by EBS snapshots which can resotre on-prem volumes
        cached volumes provide low latency access to recent data
        stored volumes - entire dataset is on-prem with scheduled backups to s3
        on-prem volume gateway is accessed  via iSCSI
    tape gateway
        tape gateway exists on-prem and uses iSCSI
        data is backed up to virtual tapes in s3 and archived tapes in glacier
    hardware appliance is available 

FSx
    for windows
        fully managed windows file system shared drive
        support SMB and NTFS
        AD integration
        built on SSD
        can be multi-AZ for HA
        backed up daily to S3
    for lustre
        linux + cluster = lustre
        used for large scale computing, HPC, machine learning
        very high performance
        seamless integration with s3
        can be used from on-prem servers

SQS
    facilitates asynchronous communication
    used to decouple applications
    unlimited throughput and number of messages in queue
    default message retention is 4 days, max 14 days
    < 10  ms response time
    256KB max message size
    may have duplicate and out of order messages (at least once delivery, best effort)
    consumers pull data
    consumers can poll up to 10 messages at a time
    consumers should delete messages using deletemessage API
    throughput is not provisioned
    encryption in flight using HTTPS
    encryption at rest via KMS
    IAM policies are used to control access to SQS API
    SQS access policies can also be used and are similar to S3 bucket policies
    Message visibility timeout
        period of timie where message becomes "invisible" to consumers after it has been polled
        message will become visible again if it is not deleted during the timeout period
        ChangeMessageAvailability can be adjusted to change this
    dead letter queue
        when a consumer fails to process a message during the timeout it is moved back to the queue
        thresholds can be set for how many time this^ can happen
        once threshold is reached, message is moved to dead letter queue
        useful for debugging
        DLQ should have high retention such as 14 days
        configured like a normal queue, other queues sppecify it as their DLQ
    delay queue
        default is 0 but can be configured up to 15 minutes
        default can be defined at queue level or inidividually with the DelaySeconds parameter
        delays messages - not sure what use case is
    FIFO Queue
        first messages in are first out
        maintains ordering
        limited to 300 msg/s or 3000 msg/s with batching
        name must end with .fifo
        deduplication ID must be specified with message
    ASG integration
        cloudwatch is commonly used  to scale ec2 instances when queue length becomes too long
        custom metric is defined in cloudwatch
        alarm is created to trigger scaling

SNS
    used when messages need to be sent to many receivers
    publisher/subscriber model
    event producer sends message to one SNS topic
    subscribers liisten to topic notifications
    each topic subscriber will get all messages
    data is pushed to subscribers
    up to 10M subscriptions
    100K topic limit
    data is not peristent in queue, will be lost if not delivered
    do not need to provision throughput
    subscribers can be basically anything
    topic publish is standard, but direct publish exists for mobile apps
    in-flight encryption with HTTPS
    at rest encryption with KMS
    AIM policies for access control
    SNS access polices can be used which are similar to s3 bucket policies
    Fan-out
        messages are sent to SNS topic first, then multiple SQS queues subscribe to it
        common to use multiple queues for multiple applications
        SNS can't send messages to FIFO queues
        commonly used with s3 notifications
            s3 notifications can only be sent to one location

Kinesis
    managed alternative to apache kafka
    great for application logs, metrics, IoT, real-time big data, streaming telemetry
    data is auto replicated to 3 AZ
    data is sent to streams, then can be analyzed by analytics, then stored with firehose
    data expires after a set number of days
    access control is delegated through IAM policies
    encryption in flight with HTTPS
    encryption at reast with KMS
    VPC endpoints can be add to make kinesis available within VPC
    throughput must be provisioned
    Kinesis Streams
        low latency streaming ingest at scale
        streams are divided into shards/partitions
        default data retention is 1 day, max is 7 days
        consumers pull data
        unlimited consumers
        data can be replayed
        multiple apps can consume the same stream
        data does not get deleted, it just expires?
        shard
            one stream is many shards
            1MB/s or 1000msg/s write per shard
            2MB/s read per shard
            billing is per shard provisioned
            messages can be batched
            records are ordered per shard
            messages get a sequence number when they are received
            API
                put records are used to send data to kinesis
                must use partition key (shard id)
                partition keys should be chosen in a way that creates distribution across shards
                ProvisionedThroughputExceeded - similar to 429 error, shard is overloaded
                    use backoff timers or more shards to fix
        consumers can use CLI, SDK, or Kinesis clent library for consumption
            KCL uses dynamoDB
    Kinesis Analytics
        used for real time analytics on streams using SQL
    Kinesis Firehose
        loads streams into s3, redshift, elasticsearch, splunk
        fully managed, automatically scales, no administration
        near real-time (60 sec latency minimum for non-full batches)
            or 32MN data at a time
        pay for amount of data going through firehose
        does not store data

MQ
    managed apache activeMQ
    doesnt scale as well as SQS/SNS
    has queues and topics
    good for migrating apps from on-prem to the cloud
    runs on dedicated machine with HA failover
    supports openwire, ws, mqtt, stomp, amqp protocols

Lambda
    runs on-demand
    pay per request and compute time
    free tier - 1M requests and 400,000 GB compute time
        $0.20 for next 1M requests
        billing also based on amout of RAM provisioned
    up to 3GB RAM can be provisioned per function
    node.js, java, python, c#, golang, ruby, powershell, customer runtime API
    can't run docker in lambda
    scales automatically
    errors are output to cloudwatch logs
    128MB to 3TB RAM available
    max execution time is 900s/15m
    max 4KB environment variables
    max 512MB disk capacity in /tmp
        /tmp should be used to load files at startup
    default 1000 concurrency executions (can be increased)
    max deployment size zipped - 50MB
        unzipped - 250MB
    Lambda@edge
        deploys lambda functions with cloudfront instead of in one AZ
        can use lambda to change cloudfront requests and responses
        viewer request (user to cloudfront), origin request (cloudfront to origin), \
            origin response (origin to cloudfront), and viewer response (cloudfront to user) can all be modified with lambda
        responses can be sent to users without communicating with origin (AWS resource)

DynamoDB
    NoSQL - not only sql
    fully managed, highly available across 3 AZ
    can handle millions of requests per second, trillions of rows, 100s of TB of storage
        massively scalable
    uses  IAM for security
    low latency
    auto scaling
    made of tables
    each table has primary key
    tables can infinite rows
    each item has attributes
    maximum item size 400KB
    DMS can be used to migrate from mongo, oracle, mysql, etc
    table must have provisioned read and write capacity units
        RCU - read capacity units
            1 RCU = 1 strongly consistent or 2 eventually consistent read(s) of 4KB/s
        WCU - write capacity units
            1 WCU = 1 write of 1KB/s
        auto-scaling can be configured to meet demand
        throughput can be temporarily exceeded with burst credits
            ProvisionedThroughException is error similar to 429 when burst credits are used
    DAX
        DynamoDB accelerator
        seamless cache for dynamodb
        fixes hotkey problem (too many reads on a specific key)
        5 minute TTL cache by default
        multi AZ
    Streams
        changes in  dynamodb are sent to dynamodb stream
        allows for reacting to changes like new users, analytics, inserting into elasticsearch
        24 hour data retention
    Transactions
        coordinates insert update and delete across multiple tables
        "all or nothing" tables
    on demand
        no capacity planning needed
        WCU RCU scales automatically
        more expensive
        helpful with unpredicatble traffic spikes
    backup/restore
        point in time like RDS
        no performance impact
    global tables
        cross region replication
        must enable streams
        useful for low latency and DR purposes

API Gateway
    can be used with lambda to create a serverless api
        also  supports http endpoints for backend
        also supports SQS foor backend
    automatically handles rate-limiting, caching, authenticaton, etc
    supports websockets
    supports versioning
    supports multiple environments such as  dev test prod
    can do authentication and authorization
    support swagger / open api import
    edge optimized 
        for  global clients
        uses cloudfront edge
        API still lives in one  region
    regional
        for regional clients
        could manually combine with cloudfront
    private
        can only be accessed from within VPC
        use resource polucy to define access
    IAM permissions can be used to delegate access
        works  well for access within your own infrastructure
        uses sigv4 cpabilities where IAM credentials are in header
    Lambda authorizer
        most common
        lambda validates token passed in header
        authentication can be cached
        used for oauth / SAML / other 3rd party auth
        lambda  returns IAM policy for user
    Cognito
        fully manages users
        API  gateway verifies identity automatically via cognito
        only helps with authentication, not authorization
        client gets token from cognito, passes token to API gateway
        user pool
            sign in functionality for app users
            integrated with API  gateway
        identity pools
            provides aws credentials to users so they can access resource directly
            integrates with user pools as identity provider
        cognito sync
            deprecated and replaced by app sync
            synchronizes mobile data with cognito
        cognito user pools
            serverless database of users for your mobile app
            uses simple username password login
            supports  email verification and MFA
            can enable federated identitites like facebook, google, saml
            sends JSON web token JWT to authenticated user
            can be integrated with API gateway
        federated identity pools
            federated identity is provideed and temporary credentials for \
                access to AWS ressources is granted
            credentials have IAM policies configured
            can be used for providing temp access to write a file to s3

SAM
    servern application model
    framework for developing and deploying serverless applications
    all configs are in YAML
        lambda functions
        dynamodb tables
        api gateway
        cognito user pools
    can help you rn lambda, api gateway, dynamodb locally
    can use codedeploy to deploy lambda functions

Redshift
    based on postgresql
    OLAP - online analytical processing
    used for analytics and data warehousing

Neptune
    fully managed graph database
    good for mapping relationships like social media
    high availability across 3 AZ
    use clustering to improve performance

ElasticSearch
    works well with nosql databases like dynamodb
    can do partial matches on keys
    big data
    can provision clusters
    works with kibana and logstash (ELK)

CLoudwatch
    10 free monitors in free tier
    ec2 detailed monitoring
        defualt is 5 minute interval
        detailed monitoring provides 1 min interval monitoring
        helps ASGs scale better
    custom metrics
        dimensions (attributes) can be added to segment metrics
        uses API PutMetricData
        use exponential backoff in case of throttle errors
    dashboards
        global in scope, can include data from different regions
        time zones can be changed
        automatic refresh can be configured
        3 dashboards up to 50 metrics are in free tier
    cloudwatch logs
        logs can be collected from beanstalk, ECS, lambda, VPC flows, API gateway, cloudtrail, cloudwatch log agents, route53
        logs can be exported to s3 or elasticsearch
        log groups can be configured
            usually represents an application
        log streams will be configured within log groups
            used to divide instances within an app
        log expiration dates can be configured
        can be viewed realtime with CLI
        agent is required for ec2 to push logs to cloudwatch
            IAM permissions required
        logs agent
            old
            only sends logs to cloudwatch
        unified agent
            new
            collects resource info in addition to logs
            centralized configuration via SSM parameter store
        cloudwatch alarms
            used to trigger notifications for any metric
            alarm states
                OK
                insufficient_data
                alarm
            period
                length of time in seconds to evaluate the metric
                high resolution custom metrics can only choose 10 or 30 seconds
        ec2 instance recovery
            cloudwatch can be used to recover a VM 
                includes private/public IPs, metadata, placement group, etc
            instance check - checks vm status
            system status - checks underlying hardware
        cloudwatch events
            scheduled with cron jobs or event patterns (matching an event) such as codepipeline changes
            triggers lambda/SQS/SNS/kinesis
            creates json document to provide info about event

Cloudtrail
    provides governance, compliance, and auditing
    enabled by default
    history of all events/API calls made within AWS account
        includes console, SDK, CLI, AWS services
    cloudtrail logs can be sent to cloudwatch
    check cloudtrail if a resource is deleted

AWS Config
    auditing and compliance
    records config changes and compliance-level over time
    stores config data in s3 to be analyzed by athena
    can  verify things like
        is there unrestricted SSH access to anything
        do any buckets have public access
        how has ALB confiig changed over time
    receive SNS alerts for changes
        can be triggered on change  or periodically
    per-region service, but can be aggregated across regions and accounts
    can create rules for auto-remediation
    $2 / active region / month - no proo-rating

STS
    Security Token Service
    grants limited and temporary access to AWS resources
    token is valid for up to one hour (must be refreshed)
    AssumeRole
        used within your own account or cross account for enhanced security
    AssumeRoleWithSAML
        return credentials for SAML users
        replaced by Amazon SSO (for ADFS / federation)
    AssumeRoldWithWebidentity
        can be used for 3rd party identity providers
        recommended to use cognito instead
    GetSessionToken
        for MFA
    Custom Identity broker can be  used if on-prem identity is not SAML compatible
    Assume Role API is called specifying which IAM role you want to assume, STS verifies with IAM \
         and returns token if approvedo
    assuming a role is kind of like su linux command
    original permissions are lost when assuming a role
    
Cognito
    usees 3rd party identity provider for authentication
        could be google, facebook, Cognito User Pool, etc
    process
        user logs in to  identity provider
        token is provided and passed to aws
        aws verifies token
        aws gets credentials fromo STS
        STS token is provided to user for access to AWS resources
    
AWS Directory Services
    managed microosoft AD
        trust can be established to on-prem AD
    AD connector
        directory gateway is estblished in AWS
        all requests are proxied to on-prem AD

Organizations
    allows you to manage multiple AWS accounts
    member accounts can only be part of one organization
    accounts may be a dept, cost center, test/dev/prod, etc
    primarily used for billing
    OUs are used for segmentation
    one master account will manage all OUs
    service control policy
        allows you to whitelist or blacklist IAM actions
        applied at account or OU level
        does not apply to master account
        applied to  all users and roles of the account, including the root
        does not affect service-linked roles
        ddoes not allow anything by default (things must be explicitly allowed)
        used for restricting access to resources or compliance
    to move accounts between orgs, they must  be removed from the original org and send an invite from the new org
    to move master account, old org must be deleted

IAM
    conditions
        aws:SourceIP  - restricts accessed to APIs based on source IP address
        aws:RequestedRegion - restricts region API calls are being made to
        can also restrict access based on tags or force MFA for certain actions
     S3
        bucket level permissions (such as listbucket) are applied to the resource (arn:aws:s3:::test)
        object level permissions (get put delete) are applied within a bucket (arn:aws:s3:::test/*)
    resource boundary
        used to restrict access of users and roles
        takes precedence over account-level permissions

Resource Access Manager
    allows you to share resources with other AWS accounts
    works with any account or within your own org
    avoids resource duplication
    vpc subnets, transit gateway, route53 resolved rules, etc can be shared

Amazon SSO
    integrated with AWS organizations
    supports saml 2.0
    integrated with on-prem AD
    provides centralized permission management
    provides centralized auditing with cloudtrail
    assumerolewithsaml differences
        SSO is  much simpler
        SSO does not use STS
        SSO login portal integrates directly with identity provider

KMS
    Key types
        symmetric
            AES-256
            no access to actual key
            single key for encrypting and decrypting
        asymmetric
            RSA & ECC key pairs
            public and private key pair
            no access to private key
            can also be used for signing
            can be used by users outside AWS who cant call APIs
    audit with cloudtrail
    customer master keys
        AWS managed service default - free
        User created key in KMS - $1 / month
        imported users keys - $1 / month - not recommended
        $0.03 per 1000 API calls
    kms can only encrypt up to 4KB data per call
        envelope encryption should be used for larger API calls
    moving an ecnrypted snapshot across regions
        create snapshot
        copy snapshot and re-encrypt with new key
        recreate volume 
    KMS keys are bound to a specific region
    has default key policies which are similar to s3 bucket policies

SSM Parameter store
    secure storage for configuration and secrets
    serverless, scalable, durable
    easy sdk
    version tracking
    get notifications via cloudwatch events
    integrated with cloudformation
    parameter policies can be attached that expire credentials or force updating
    aws ssm get-parameters --names [name of key] --with-decryption
        used to get keys from SSM in decrypted format
    boto3 - python library fer keys?

AWS Secrets Manager
    newer service used for storing secrets
    can force rotation of secrets
    can automate generation of secrets on rotation
    integrated with RDS
        mostly intended for RDS?
    secrets are managed via KMS

CloudHSM
    provisioned hardware module
    FIPS 140-2 level 3 compliance
    can use clusters are multi AZ for HA
    supports symmetric and asymmetric encryption
    must use cloudhsm client software
    redshift support
    good for SSE-C rencryption

Shield
    shield standard - free
        protects against layer 3/4 attacks (syn/udp flood, felecton, etc)
    shield advanced
        $3K / mo / org
        protects against sophisticated attacks on EC2, ELB, cloudfront, glocal accelerator, route 53
        24/7 ddos response team is included

WAF
    layer 7 firewall for web apps
    can be deployed on ALB, API gateway, and cloudfront
    rules can be IPs, HTTP headers, HTTP body, or URI strings
    protects froom SQL Injection and XSS
    geo-matching can be used to block specific countries
    rate-based rules can be used to help prevent ddos

AWS firewall manager
    manage rules accross all accounts in an AWS org
    manages WAF, shield advanced, and security groups for EC2

VPC
    default VPC has internet connectivity and all instances have a public IP
    subnets are tied to an availability zone
    5 IPs are reserved in each subnet
        network address, VPC router, mapping to DNS, reserved for future use, broadcast address
    auto-assign public IP setting is used  to automatically assign a public in addition to a private IP for each instance
    NAT instances
        allows instances in private subnets to talk to the internet
        must be launched in a public subnet
        must have elastic IP
        must disable EC2 flag - src/dest check
        route table must route from private subnets to nat instance
    NAT Gateway
        aws managed nat
        pay by hour
        created in a specific AZ, will use elastic IP
        can only be used by instances in other subnets
        requires an internet gateway
        5Gb bandwidth which auto-scales up to 45Gb
        no security groups to manage
        resilient wiithin a single AZ
            must setup multiple in mutliple AZ for fault tolerance
    DNS resolution
        enableDnsSupport - DNS resolution setting
            default is true
            determines is dns resolution is enabled for the VPC
            if true queries will go to 169.254.169.253
        enableDnsHostname - DNS hostname setting
            false by default for new VPCs, true by default for default VPC
            no affect unless enableDnsSupport=true
            when true, public hostname is assigned to EC2 instance if it has public ip
        ^both must be enabled if you want to use custom dns names in a private zone in route53
    NACL
        NACL rules are evaluated before security groups for inbound traffic
        security groups are stateful and allow all outbound traffic
        NACL can be used for outbound traffic as well
        NACLs use ACE numbers (1-32766), rules are evaluated until a match is found
        implicit deny all at end of NACL
        new NACLs will deny all by default
    VPC Peering
        VPC peering is not transitive
            If A connect to B and B to C, A and C are not connected
        cannot have overlapping IP ranges
        can work inter-region and cross acount
        routes need to be maually added
    VPC Endpoint
        used to talk directly to AWS services via private network
        two types interface and gateway
            interface
                provisions ENI (private IP) as entry point
                works with most aws services
            gateway
                provisioned target and route table is automatically updated
                used for s3 and dynamodb
    Flow Logs
        can capture VPC, Subnet, and ENI flows
        can be sent directly to s3 or cloudwatch logs
        athena or cloudwatch log insights can be used to view flows
    Bastion Hosts
        linux host in public subnet used as a proxy to reach internal servers
        basically a jumpbox
    Virtual private Gateway
        vpn concentrator within AWS
        can customize the ASN
        attached to VPC where you want to make site to site VPN
        customer gateway - on-prem vpn endpoint
        customer gateway and amazon virtual private gateway must be configured
            then VPN connection is configured using these 
    Direct connect
        private connection
        virtual private gateway needs to be configured in desired VPC
        used to increase throughput, more consistent networking, etc
        direct connect gateway can be configured to connect to multiple VPCs
        1 or 10Gb are options
        hosted connections
            utilizes AWS direct connect partners
            more speed options
            can be added or removed on-demand
        data is transit is not encrypted by default
            use with VPN encryption
    Egress only internet gateway
        ipv6 only
        similar to nat but  for ipv6
    AWS private link (aka VPC endpoint services)
        used to connect to many VPCs such as customer VPCs
        load balancer is created in your service VPC
        ENI is created in customer VPC
    EC2 classic and AWS classiclink
        ec2 classic - instances run in shared network with other customers
        amazon vpc - logically isolates instances
        both are deprecated
    VPN cloudhub
        similar to sdwan
        enables backup connectivity between sites through AWS VPN
        hub and spoke model
    Transit Gateway
        star topology for conneecting all VPC and network resources
        supports multicast
    Network Costs
        inbound traffic is free
        private to private IP within an AZ is free
        $0.02/Gb for public to public communication between AZs
        $0.01/Gb for private to private communication between AZs
        $0.02/Gb for inter-region traffic

Database migration service
    runs on ec2 instance
    supports like-to-like and mixed  migrations
        ms to ms or ms to aurora
    supports continuous data replication using CDC
    sources
        oracle, mysql, mariadb, ms, postgresql, SAP, azure, amazon rds, s3
    targets
        oracle, ms, mysql, mariadb,  progesql, SAP, amazon rds, amazon redshift, dyanmodb, s3, etc
    SCT schema conversion tool required  to migrate from one db to another

On-prem AWS stuff
    VMs can be imported and exported to/from ec2 and on-prem hypervisor
    application discovery service
        gathers inifo about on-prem servers to help plan migration
        looks ar server utilization and dependency mappings
        track via migration hub
    SMS server migration service
        replicates on-prem servers to AWS
    Datasync
        helps migrate large amounts of data to aws
        used for on-prem NFS/SMB server migrations
        can replicate data weekly, daily, or hourly
    
AWS batch
    multinode parallel jobs
        enables you to run single jobs that span multiple ec2 instances
    easily schedule jobs and lauch ec2 instances
AWS parallelcluster
    open source cluster management tool
    configured with text files
    automates creation of VPC, subnet, cluster type, and instance types

CodeBuild
    aws version of jenkins
    can run code tests and deliver results
    builds and releases software
CodeCommit
    similar to github
AWS Codepipeline
    manages full CICD pipeline
        codecommit, codebuild, beanstalk, codedeploy, etc

Cloudformation
    similar to terraform
    declarative way of describing what infra should look like
    define sec group, ec2, EIPs, S3, ELB and it will be deployed for you
    stackset
        allows you to create updeate or delete cloudformation across multple accounts
    
ECS - elastic container service
    ECS core - ecs on user-provisioned EC2 instances
    fargate - ECS on AWS managed compute
        scales automatically
    EKS - ECS on AWS-powered kubernetes
    IAM security is supported
    can be used with ALB and  port mapping feature
        allows you to run muiltiple containers on the same instance
    uses ecs config file to assign instances to an ec2 cluster, pull images, log with cloudwatch, enable IAM roles
    ECR - elastic container registry
        repo for container images
    ECS Task role
        similar to IAM role but for an individual container rather than the EC2 instance it runs on
    
EKS - elastic kubernetes service
    managed kubernetes
    primary use case if migrating from an existing kube environment

Step Functions
    JSON state machine
    orchestrates lambda functions
    can integrate with EC2, ECS, on-prem servers, API gateway
SWF - simple workflow service
    runs on ec2
    workflow 
    similar to step functions but older version

EMR
    elastic mapreduce
    helps create hadoop clusters
    emr clusters can be made of hundreds of ec2 instances
    auto-scales
    can integrate with spot instances

Glue
    managed ETL - extract transport load
    automates time coonsuming steps for data analytics
    serverless
    automates code generation for apache spark
    works with aurora, rds, redshift, s3

opsworks
    managed chef and puppet

Elastic transcoder
    converts media files stored in s3 to various formats
    intended to optimize files for different mediums (mobile/tv/etc)

workspaces
    cloud VDI
    integrates with microsoft AD
    supports linux too

appsync
    stores mobile and webapp data in real time
    uses graphql
        facebook technology
    integrates with dynamodb/lambda
    replaces cognito sync

well architected framework
    operational excellence
        operations/infra as code
        annotate documentation
        make frequent small reversible changes
        refine operations procedures frequently
        anticipate failure
        learn from operational failures
    security
        strong identity foundation
            principle of least privilege
        enable traceability/logging
        apply security at all layers
        automate security best practices
        protect data in transit and at rest
        reduce or eliminate need for direct access to data
        prepare for security events
    reliability
        test recovery procedures
        automatically recover from failure
        scale horizontally
        stop guessing capacity
            use autoscaling
        manage changes via automation
    performance efficiency
        democratize advanced technologies
            enables you to focus on product development
        go global in minutes
        use serverless architectures
        expirement more often
        mechanical sympathy
            be aware of all aws services
    cost optimization
        adopt a consumption mode
            pay only for what you use
        measure overall efficiency
        stop spending on DC operations (use hosting)
        analyze and attribute expenditure
        use managed and application  level services to reduce cost of ownership
